{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from typing import Optional\n",
    "from pydantic import BaseModel\n",
    "from beanie import Document\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = yaml.load(open('config.yaml', 'r'), Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributePiece(BaseModel):\n",
    "    polyph: list[int]\n",
    "    rythm: list[int]\n",
    "\n",
    "class ContentPiece(BaseModel):\n",
    "    name: Optional[int| str]\n",
    "    bar_pos: list\n",
    "    content: list[dict]\n",
    "\n",
    "class DataPiece(Document):\n",
    "    dataset: Optional[str]\n",
    "    version: Optional[str]\n",
    "    content: ContentPiece\n",
    "    attr_cls: AttributePiece\n",
    "\n",
    "    class Settings:\n",
    "        name = 'dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_load(path):\n",
    "  return pickle.load(open(path, 'rb'))\n",
    "\n",
    "def pickle_dump(obj, f):\n",
    "  pickle.dump(obj, open(f, 'wb'), protocol=pickle.HIGHEST_PROTOCOL)\n",
    "  \n",
    "def is_file(filename, format):\n",
    "  if format in str(filename):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def npint2native(events: list):\n",
    "  for event in events: \n",
    "    e_value = event['value']\n",
    "    if type(e_value) == np.int64 or type(e_value) == np.int32:\n",
    "        event['value'] = int(e_value)\n",
    "\n",
    "  return events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# src_dir = r'agent\\remi_dataset'\n",
    "# leaf_polyph = r'agent\\remi_dataset\\attr_cls\\polyph'\n",
    "# leaf_rhythm = r'agent\\remi_dataset\\attr_cls\\rhythm'\n",
    "# dataset_name = 'AILabs.tw-Pop1K7'\n",
    "\n",
    "# idx = 0\n",
    "# for filename in tqdm(os.listdir(src_dir), desc='Transfering to DB ... '):\n",
    "#     name = filename\n",
    "#     filename = os.path.join(src_dir, filename)\n",
    "#     if not is_file(filename, format='.pkl'):\n",
    "#         continue\n",
    "\n",
    "#     # Gather a file that contain events. \n",
    "#     bar_pos, events = pickle_load(filename)\n",
    "#     events = npint2native(events=events)\n",
    "#     content_piece = ContentPiece(name=name, bar_pos=bar_pos, content=events)\n",
    "\n",
    "#     # Go to leaf dir to gather appropriate attribute-files to the file. \n",
    "#     polyph_file = os.path.join(leaf_polyph, name)\n",
    "#     rhythm_file = os.path.join(leaf_rhythm, name)\n",
    "\n",
    "#     pol_content = pickle_load(polyph_file)\n",
    "#     rhy_content = pickle_load(rhythm_file)\n",
    "\n",
    "#     attr_piece = AttributePiece(polyph=pol_content, rythm=rhy_content)\n",
    "    \n",
    "#     # Store to DB. \n",
    "#     data_piece = DataPiece( \n",
    "#                            dataset=dataset_name, \n",
    "#                            version='remi_data_v0',\n",
    "#                            content=content_piece, \n",
    "#                            attr_cls=attr_piece)\n",
    "    \n",
    "#     await data_piece.insert()\n",
    "\n",
    "#     idx += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_local_db(data_collection, tgt_dir, tgt_leaf_polyph, tgt_leaf_rhythm): \n",
    "    for document in tqdm(data_collection, desc='Intializing Local DB'): \n",
    "        id, dataset, version, content, attr_cls = document.values()\n",
    "\n",
    "        name, bar_pos, events = content.values()\n",
    "        polyph, rhythm = attr_cls.values()\n",
    "\n",
    "        tgt_filename = os.path.join(tgt_dir, name)\n",
    "        tgt_polyph = os.path.join(tgt_leaf_polyph, name)\n",
    "        tgt_rhythm = os.path.join(tgt_leaf_rhythm, name)\n",
    "\n",
    "        pickle_dump((bar_pos, events), f=tgt_filename)\n",
    "        pickle_dump(polyph, tgt_polyph)\n",
    "        pickle_dump(rhythm, tgt_rhythm)\n",
    "\n",
    "def remove_samples_from_local_db(tgt_dir, tgt_leaf_polyph, tgt_leaf_rhythm, k_samples=10):\n",
    "    samples = [sample for sample in os.listdir(tgt_dir) if '.pkl' in sample]\n",
    "    if len(samples) <= k_samples:\n",
    "        print('Small than expected') \n",
    "        return \n",
    "    rand_idx = np.random.choice(len(samples), k_samples, replace=False)\n",
    "    for idx in tqdm(range(k_samples), desc='Removing files'):\n",
    "        rand_sample = samples[rand_idx[idx]]\n",
    "        rand_file = os.path.join(tgt_dir, rand_sample)\n",
    "        rand_polyph = os.path.join(tgt_leaf_polyph, rand_sample)\n",
    "        rand_rhythm = os.path.join(tgt_leaf_rhythm, rand_sample)\n",
    "\n",
    "        os.remove(rand_file)\n",
    "        os.remove(rand_polyph)\n",
    "        os.remove(rand_rhythm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Intializing Local DB: 100%|██████████| 3/3 [00:00<00:00, 272.74it/s]\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "\"\"\" \n",
    "    1. The function takes n samples and stores those in the local storage for usage. \n",
    "    2. The timer start counting delta-time. \n",
    "    3. When the timer meets a deadline, the function: \n",
    "        3.1. Eliminate n//k samples in the local storage. \n",
    "        3.2. Takes n _samples and stores those in the local storage. \n",
    "    4. Repeat step 1-3. \n",
    "\"\"\"\n",
    "tgt_dir = r'data\\examples\\samples'\n",
    "tgt_leaf_polyph = r'data\\examples\\samples\\polyph'\n",
    "tgt_leaf_rhythm = r'data\\examples\\samples\\rhythm'\n",
    "\n",
    "n_samples = 50\n",
    "k_samples = 45\n",
    "period_time = 60 * 3\n",
    "\n",
    "from beanie import init_beanie\n",
    "from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase\n",
    "\n",
    "client = AsyncIOMotorClient(\n",
    "    \"mongodb://localhost:27017\"\n",
    ")\n",
    "\n",
    "# Initialize beanie with the Product document class and a database\n",
    "await init_beanie(database=client.NeuralNotes, document_models=[DataPiece])\n",
    "data_collection = await DataPiece.find().aggregate([{'$sample': {'size': 3}}]).to_list()\n",
    "initialize_local_db(data_collection, tgt_dir, tgt_leaf_polyph, tgt_leaf_rhythm)\n",
    "# with open(r'log.txt', 'a') as file: \n",
    "#     moment = time.ctime()\n",
    "#     samples = [sample for sample in os.listdir(tgt_dir) if '.pkl' in sample]\n",
    "#     sentence = f'[{moment}] Adding {k_samples} from {tgt_dir}, there are {len(samples)} left. \\n'\n",
    "#     file.write(sentence)\n",
    "# while True: \n",
    "#     remove_samples_from_local_db(tgt_dir, tgt_leaf_polyph, tgt_leaf_rhythm, k_samples=k_samples)\n",
    "#     with open(r'log.txt', 'a') as file: \n",
    "#         moment = time.ctime()\n",
    "#         samples = [sample for sample in os.listdir(tgt_dir) if '.pkl' in sample]\n",
    "#         sentence = f'[{moment}]: Remove {k_samples} from {tgt_dir}, there are {len(samples)} left. \\n'\n",
    "#         file.write(sentence)\n",
    "#     data_collection = await DataPiece.find().aggregate([{'$sample': {'size': n_samples}}]).to_list()\n",
    "#     initialize_local_db(data_collection, tgt_dir, tgt_leaf_polyph, tgt_leaf_rhythm)\n",
    "#     with open(r'log.txt', 'a') as file: \n",
    "#         moment = time.ctime()\n",
    "#         samples = [sample for sample in os.listdir(tgt_dir) if '.pkl' in sample]\n",
    "#         sentence = f'[{moment}] Adding {k_samples} from {tgt_dir}, there are {len(samples)} left. \\n'\n",
    "#         file.write(sentence)\n",
    "#     time.sleep(period_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Removing files: 100%|██████████| 1/1 [00:00<00:00, 499.68it/s]\n"
     ]
    }
   ],
   "source": [
    "tgt_dir = r'data\\resource'\n",
    "tgt_leaf_polyph = r'data\\resource\\attr_cls\\polyph'\n",
    "tgt_leaf_rhythm = r'data\\resource\\attr_cls\\rhythm'\n",
    "n_samples = 100\n",
    "k_samples = 95\n",
    "# k_samples = len(os.listdir(tgt_dir)) - 1\n",
    "remove_samples_from_local_db(tgt_dir, tgt_leaf_polyph, tgt_leaf_rhythm, k_samples=k_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3,  5,  6, 16,  8,  4,  7,  6, 17, 15])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(20, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_dir = r'datas\\examples\\samples'\n",
    "tgt_leaf_polyph = r'datas\\examples\\samples\\polyph'\n",
    "tgt_leaf_rhythm = r'datas\\examples\\samples\\rhythm'\n",
    "\n",
    "data_collection = await DataPiece.find().aggregate([{'$sample': {'size': 3}}]).to_list()\n",
    "for document in data_collection: \n",
    "    id, dataset, version, content, attr_cls = document.values()\n",
    "\n",
    "    name, bar_pos, events = content.values()\n",
    "    polyph, rhythm = attr_cls.values()\n",
    "\n",
    "    tgt_filename = os.path.join(tgt_dir, name)\n",
    "    tgt_polyph = os.path.join(tgt_leaf_polyph, name)\n",
    "    tgt_rhythm = os.path.join(tgt_leaf_rhythm, name)\n",
    "\n",
    "    pickle_dump((bar_pos, events), f=tgt_filename)\n",
    "    pickle_dump(polyph, tgt_polyph)\n",
    "    pickle_dump(rhythm, tgt_rhythm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collection[0]\n",
    "id, dataset, version, content, arr_cls = data_collection[0].values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "name, bar_pos, events = content.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1023, 1060, 1208, 1230, 1325, 1438, 1539, 1635, 1668, 172, 253, 255, 26, 269, 27, 28, 286, 29, 30, 31, 361, 463, 628, 666, 718, 806, 889]\n",
      "[26, 27, 28, 29, 30, 31, 172, 253, 255, 269, 286, 361, 463, 628, 666, 718, 806, 889, 1023, 1060, 1208, 1230, 1325, 1438, 1539, 1635, 1668]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1669"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_filename_from_storage(data_dir):\n",
    "    # The dataset receive integer index as an input, therefore, a name of a file should be change. \n",
    "    file_storage = os.listdir(data_dir)\n",
    "    files = [int(pkl_file.split('.')[0]) for pkl_file in file_storage if '.pkl' in pkl_file]\n",
    "    files.sort()\n",
    "    filename = files[-1]  + 1\n",
    "    return filename\n",
    "\n",
    "\n",
    "get_filename_from_storage(data_dir='results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import os, pickle\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from models.data_piece import ContentPiece, AttributePiece, DataPiece\n",
    "\n",
    "\n",
    "from beanie import init_beanie, Document\n",
    "from motor.motor_asyncio import AsyncIOMotorClient, AsyncIOMotorDatabase\n",
    "\n",
    "from .utils import pickle_load, pickle_dump\n",
    "\n",
    "src_dir = r'agent\\remi_dataset'\n",
    "leaf_polyph = r'agent\\remi_dataset\\attr_cls\\polyph'\n",
    "leaf_rhythm = r'agent\\remi_dataset\\attr_cls\\rhythm'\n",
    "dataset_name = 'AILabs.tw-Pop1K7'\n",
    "\n",
    "tgt_dir = r'dataset\\resource'\n",
    "tgt_leaf_polyph = r'dataset\\resource\\polyph'\n",
    "tgt_leaf_rhythm = r'dataset\\resource\\rhythm'\n",
    "\n",
    "def is_file(filename, format):\n",
    "  if format in str(filename):\n",
    "    return True\n",
    "  return False\n",
    "\n",
    "def npint2native(events: list):\n",
    "  for event in events: \n",
    "    e_value = event['value']\n",
    "    if type(e_value) == np.int64 or type(e_value) == np.int32:\n",
    "        event['value'] = int(e_value)\n",
    "\n",
    "  return events\n",
    "\n",
    "async def init():\n",
    "    # Create Motor client\n",
    "    client = AsyncIOMotorClient(\n",
    "        \"mongodb://user:pass@host:27017\"\n",
    "    )\n",
    "\n",
    "    # Initialize beanie with the Product document class and a database\n",
    "    await init_beanie(database=client.db_name, document_models=[DataPiece])\n",
    "\n",
    "\n",
    "async def storing_lcl_db(dataset_name=None, version=None):\n",
    "   if dataset_name:\n",
    "      dataset_name = dataset_name\n",
    "   else: \n",
    "      dataset_name = 'Unknown'\n",
    "\n",
    "   if version: \n",
    "      version = version \n",
    "   else: \n",
    "      version = 'Unknown'\n",
    "\n",
    "   for filename in tqdm(os.listdir(src_dir), desc='Transfering to DB ... '):\n",
    "    name = filename\n",
    "    filename = os.path.join(src_dir, filename)\n",
    "    if not is_file(filename, format='.pkl'):\n",
    "        continue\n",
    "\n",
    "    # Gather a file that contain events. \n",
    "    bar_pos, events = pickle_load(filename)\n",
    "    events = npint2native(events=events)\n",
    "    content_piece = ContentPiece(name=name, bar_pos=bar_pos, content=events)\n",
    "\n",
    "    # Go to leaf dir to gather appropriate attribute-files to the file. \n",
    "    polyph_file = os.path.join(leaf_polyph, name)\n",
    "    rhythm_file = os.path.join(leaf_rhythm, name)\n",
    "\n",
    "    pol_content = pickle_load(polyph_file)\n",
    "    rhy_content = pickle_load(rhythm_file)\n",
    "\n",
    "    attr_piece = AttributePiece(polyph=pol_content, rythm=rhy_content)\n",
    "    \n",
    "    # Store to DB. \n",
    "    data_piece = DataPiece( \n",
    "                           dataset=dataset_name, \n",
    "                           version=version,\n",
    "                           content=content_piece, \n",
    "                           attr_cls=attr_piece)\n",
    "    \n",
    "    await data_piece.insert()\n",
    "\n",
    "\n",
    "def initialize_local_db(data_collection, tgt_dir, tgt_leaf_polyph, tgt_leaf_rhythm): \n",
    "    for document in tqdm(data_collection, desc='Intializing Local DB'): \n",
    "        id, dataset, version, content, attr_cls = document.values()\n",
    "\n",
    "        name, bar_pos, events = content.values()\n",
    "        polyph, rhythm = attr_cls.values()\n",
    "\n",
    "        tgt_filename = os.path.join(tgt_dir, name)\n",
    "        tgt_polyph = os.path.join(tgt_leaf_polyph, name)\n",
    "        tgt_rhythm = os.path.join(tgt_leaf_rhythm, name)\n",
    "\n",
    "        pickle_dump((bar_pos, events), f=tgt_filename)\n",
    "        pickle_dump(polyph, tgt_polyph)\n",
    "        pickle_dump(rhythm, tgt_rhythm)\n",
    "\n",
    "def remove_samples_from_local_db(tgt_dir, tgt_leaf_polyph, tgt_leaf_rhythm, k_samples=10):\n",
    "    samples = [sample for sample in os.listdir(tgt_dir) if '.pkl' in sample]\n",
    "    if len(samples) <= k_samples:\n",
    "        print('Small than expected') \n",
    "        return \n",
    "    rand_idx = np.random.choice(len(samples), k_samples, replace=False)\n",
    "    for idx in tqdm(range(k_samples), desc='Removing files'):\n",
    "        rand_sample = samples[rand_idx[idx]]\n",
    "        rand_file = os.path.join(tgt_dir, rand_sample)\n",
    "        rand_polyph = os.path.join(tgt_leaf_polyph, rand_sample)\n",
    "        rand_rhythm = os.path.join(tgt_leaf_rhythm, rand_sample)\n",
    "\n",
    "        os.remove(rand_file)\n",
    "        os.remove(rand_polyph)\n",
    "        os.remove(rand_rhythm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agent.utils import word_to_event, write_midi\n",
    "\n",
    "tgt_dir = r'data\\examples\\samples'\n",
    "tgt_leaf_polyph = r'data\\examples\\samples\\polyph'\n",
    "tgt_leaf_rhythm = r'data\\examples\\samples\\rhythm'\n",
    "\n",
    "sample = pickle_load(os.path.join(tgt_dir, '838.pkl'))\n",
    "remi_vocab = pickle_load(r'agent\\pickles\\remi_vocab_v2.pkl')\n",
    "word2event = remi_vocab[1]\n",
    "\n",
    "events = word_to_event(word2event, sample[1])\n",
    "events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'Tempo', 'value': 119}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 89}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 86}\n",
      "{'name': 'Tempo', 'value': 95}\n",
      "{'name': 'Tempo', 'value': 146}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 182}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 182}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 182}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 161}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 182}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 182}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 161}\n",
      "{'name': 'Tempo', 'value': 182}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 161}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 182}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 182}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 182}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 161}\n",
      "{'name': 'Tempo', 'value': 176}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 167}\n",
      "{'name': 'Tempo', 'value': 170}\n",
      "{'name': 'Tempo', 'value': 161}\n",
      "{'name': 'Tempo', 'value': 146}\n",
      "{'name': 'Tempo', 'value': 137}\n",
      "{'name': 'Tempo', 'value': 134}\n",
      "{'name': 'Tempo', 'value': 140}\n",
      "{'name': 'Tempo', 'value': 137}\n",
      "{'name': 'Tempo', 'value': 131}\n",
      "{'name': 'Tempo', 'value': 137}\n",
      "{'name': 'Tempo', 'value': 128}\n",
      "{'name': 'Tempo', 'value': 134}\n",
      "{'name': 'Tempo', 'value': 128}\n"
     ]
    }
   ],
   "source": [
    "for ev in sample[1]: \n",
    "    if 'Tempo' in ev['name']:\n",
    "        print(ev)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicgen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
